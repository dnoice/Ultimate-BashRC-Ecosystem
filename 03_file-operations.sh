#!/bin/bash
# =============================================================================
# ULTIMATE BASHRC ECOSYSTEM - FILE OPERATIONS MODULE
# File: 03_file-operations.sh
# =============================================================================
# This module provides revolutionary file operations with AI-like intelligence,
# learning capabilities, automated organization, real-time monitoring, and
# advanced file management that goes far beyond traditional tools.
# =============================================================================

# Module metadata
declare -r FILE_OPS_MODULE_VERSION="2.1.0"
declare -r FILE_OPS_MODULE_NAME="Ultimate File Operations"
declare -r FILE_OPS_MODULE_AUTHOR="Ultimate Bashrc Ecosystem"

# Module initialization
echo -e "${BASHRC_CYAN}ğŸ“ Loading Ultimate File Operations...${BASHRC_NC}"

# =============================================================================
# INTELLIGENT FILE ORGANIZATION SYSTEM
# =============================================================================

# AI-like file organizer that learns from your patterns
organize() {
    local usage="Usage: organize [OPTIONS] [DIRECTORY]
    
ğŸ§  Intelligent File Organization System
Options:
    -m, --mode MODE     Organization mode (auto, type, date, size, project)
    -r, --recursive     Organize subdirectories recursively  
    -d, --dry-run       Show what would be organized without doing it
    -l, --learn         Learn from current organization
    -s, --smart         Use AI-like smart categorization
    -c, --custom FILE   Use custom organization rules file
    -v, --verbose       Verbose output
    --undo             Undo last organization (if logged)
    -h, --help          Show this help
    
Modes:
    auto     - Automatic smart organization (default)
    type     - Organize by file type/extension
    date     - Organize by modification date
    size     - Organize by file size categories
    project  - Organize by detected project structure
    
Features: Pattern learning, intelligent categorization, undo support, project detection"

    local target_dir="."
    local org_mode="auto"
    local recursive_mode=false
    local dry_run_mode=false
    local learn_mode=false
    local smart_mode=false
    local custom_rules=""
    local verbose_mode=false
    local undo_mode=false
    
    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -m|--mode)      org_mode="$2"; shift 2 ;;
            -r|--recursive) recursive_mode=true; shift ;;
            -d|--dry-run)   dry_run_mode=true; shift ;;
            -l|--learn)     learn_mode=true; shift ;;
            -s|--smart)     smart_mode=true; shift ;;
            -c|--custom)    custom_rules="$2"; shift 2 ;;
            -v|--verbose)   verbose_mode=true; shift ;;
            --undo)         undo_mode=true; shift ;;
            -h|--help)      echo "$usage"; return 0 ;;
            -*)             echo "Unknown option: $1" >&2; return 1 ;;
            *)              target_dir="$1"; shift ;;
        esac
    done
    
    [[ ! -d "$target_dir" ]] && { echo "âŒ Directory '$target_dir' not found"; return 1; }
    
    # Set up organization analytics and history
    local org_dir="$HOME/.bash_file_operations"
    local org_history="$org_dir/organize_history.log"
    local org_patterns="$org_dir/organization_patterns.json"
    local org_rules="$org_dir/custom_rules.json"
    mkdir -p "$org_dir"
    
    # Handle undo mode
    if [[ "$undo_mode" == "true" ]]; then
        undo_organization "$target_dir" "$org_history"
        return $?
    fi
    
    echo -e "ğŸ§  ${BASHRC_CYAN}Intelligent File Organization Starting...${BASHRC_NC}"
    echo -e "ğŸ“ ${BASHRC_YELLOW}Directory:${BASHRC_NC} $(realpath "$target_dir")"
    echo -e "ğŸ¯ ${BASHRC_YELLOW}Mode:${BASHRC_NC} $org_mode"
    
    local start_time=$(date +%s)
    
    # Analyze directory first
    echo -e "ğŸ” ${BASHRC_CYAN}Analyzing directory structure...${BASHRC_NC}"
    local analysis_result=$(analyze_directory_for_organization "$target_dir")
    
    # Learn from existing organization if requested
    if [[ "$learn_mode" == "true" ]]; then
        echo -e "ğŸ“ ${BASHRC_CYAN}Learning from current organization...${BASHRC_NC}"
        learn_organization_patterns "$target_dir" "$org_patterns"
    fi
    
    # Load custom rules if provided
    local custom_rule_set=""
    if [[ -n "$custom_rules" && -f "$custom_rules" ]]; then
        custom_rule_set=$(cat "$custom_rules")
        echo -e "ğŸ“‹ ${BASHRC_CYAN}Loading custom rules from $custom_rules${BASHRC_NC}"
    fi
    
    # Execute organization based on mode
    local files_processed=0
    local operations_log=$(mktemp)
    
    case "$org_mode" in
        auto|smart)
            files_processed=$(organize_smart "$target_dir" "$recursive_mode" "$dry_run_mode" "$verbose_mode" "$operations_log" "$org_patterns" "$analysis_result")
            ;;
        type)
            files_processed=$(organize_by_type "$target_dir" "$recursive_mode" "$dry_run_mode" "$verbose_mode" "$operations_log")
            ;;
        date)
            files_processed=$(organize_by_date "$target_dir" "$recursive_mode" "$dry_run_mode" "$verbose_mode" "$operations_log")
            ;;
        size)
            files_processed=$(organize_by_size "$target_dir" "$recursive_mode" "$dry_run_mode" "$verbose_mode" "$operations_log")
            ;;
        project)
            files_processed=$(organize_by_project "$target_dir" "$recursive_mode" "$dry_run_mode" "$verbose_mode" "$operations_log")
            ;;
        *)
            echo "âŒ Unknown organization mode: $org_mode"
            rm -f "$operations_log"
            return 1
            ;;
    esac
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    # Log the operation
    if [[ "$dry_run_mode" == "false" && -s "$operations_log" ]]; then
        echo "$(date -Iseconds)|$org_mode|$(realpath "$target_dir")|$files_processed|$duration" >> "$org_history"
        cat "$operations_log" >> "$org_history"
        echo "---" >> "$org_history"
    fi
    
    echo -e "\nâœ… ${BASHRC_GREEN}Organization completed!${BASHRC_NC}"
    echo -e "ğŸ“Š ${BASHRC_YELLOW}Files processed:${BASHRC_NC} $files_processed"
    echo -e "â±ï¸  ${BASHRC_YELLOW}Duration:${BASHRC_NC} ${duration}s"
    
    if [[ "$dry_run_mode" == "true" ]]; then
        echo -e "ğŸ’¡ ${BASHRC_CYAN}This was a dry run - no files were actually moved${BASHRC_NC}"
    else
        echo -e "ğŸ“ ${BASHRC_CYAN}Operation logged for undo support${BASHRC_NC}"
    fi
    
    rm -f "$operations_log"
}

# Analyze directory for intelligent organization
analyze_directory_for_organization() {
    local target_dir="$1"
    local temp_analysis=$(mktemp)
    
    # Collect file statistics
    local total_files=$(find "$target_dir" -maxdepth 1 -type f | wc -l)
    local total_dirs=$(find "$target_dir" -maxdepth 1 -type d | wc -l)
    local largest_file=$(find "$target_dir" -maxdepth 1 -type f -printf '%s %p\n' 2>/dev/null | sort -rn | head -1)
    local oldest_file=$(find "$target_dir" -maxdepth 1 -type f -printf '%T@ %p\n' 2>/dev/null | sort -n | head -1)
    local newest_file=$(find "$target_dir" -maxdepth 1 -type f -printf '%T@ %p\n' 2>/dev/null | sort -rn | head -1)
    
    # Analyze file types
    local file_types=$(find "$target_dir" -maxdepth 1 -type f -name "*.*" | \
                      sed 's/.*\.//' | tr '[:upper:]' '[:lower:]' | \
                      sort | uniq -c | sort -rn)
    
    # Detect project indicators
    local project_indicators=""
    [[ -f "$target_dir/package.json" ]] && project_indicators="$project_indicators nodejs"
    [[ -f "$target_dir/requirements.txt" || -f "$target_dir/setup.py" ]] && project_indicators="$project_indicators python"
    [[ -f "$target_dir/Cargo.toml" ]] && project_indicators="$project_indicators rust"
    [[ -f "$target_dir/pom.xml" ]] && project_indicators="$project_indicators java"
    [[ -f "$target_dir/Makefile" ]] && project_indicators="$project_indicators makefile"
    [[ -f "$target_dir/.git" || -d "$target_dir/.git" ]] && project_indicators="$project_indicators git"
    
    # Create analysis summary
    cat > "$temp_analysis" << EOF
{
  "total_files": $total_files,
  "total_directories": $((total_dirs - 1)),
  "largest_file": "${largest_file#* }",
  "project_indicators": "$project_indicators",
  "dominant_file_types": [
$(echo "$file_types" | head -5 | awk '{printf "    \"%s\",\n", $2}' | sed '$ s/,$//')
  ]
}
EOF
    
    echo "$temp_analysis"
}

# REVOLUTIONARY AI-Level Smart Organization with Machine Learning-Style Intelligence
organize_smart() {
    local target_dir="$1"
    local recursive="$2"
    local dry_run="$3"
    local verbose="$4"
    local operations_log="$5"
    local patterns_file="$6"
    local analysis_file="$7"
    
    local files_processed=0
    local find_opts=(-maxdepth 1)
    [[ "$recursive" == "true" ]] && find_opts=()
    
    echo -e "ğŸ§  ${BASHRC_CYAN}Initializing Advanced AI Classification Engine...${BASHRC_NC}"
    
    # Initialize Intelligence System
    local intelligence_dir="$HOME/.bash_file_operations/intelligence"
    mkdir -p "$intelligence_dir"/{models,training_data,user_feedback,content_analysis}
    
    local user_model="$intelligence_dir/models/user_classification_model.json"
    local content_cache="$intelligence_dir/content_analysis/cache.db"
    local learning_log="$intelligence_dir/training_data/learning_$(date +%Y%m%d).log"
    
    # Load or create user intelligence model
    load_or_create_user_model "$user_model"
    
    # Advanced multi-dimensional feature extraction
    echo -e "ğŸ”¬ ${BASHRC_CYAN}Performing multi-dimensional file analysis...${BASHRC_NC}"
    local temp_features=$(mktemp)
    
    # Process files with advanced intelligence
    while IFS= read -r -d '' file; do
        [[ ! -f "$file" ]] && continue
        
        local filename=$(basename "$file")
        echo -e "ğŸ” Analyzing: $filename" >&2
        
        # PHASE 1: Multi-Dimensional Feature Extraction
        local features=$(extract_advanced_features "$file" "$intelligence_dir")
        echo "$file|$features" >> "$temp_features"
        
    done < <(find "$target_dir" "${find_opts[@]}" -type f -print0)
    
    echo -e "ğŸ¤– ${BASHRC_CYAN}Running Advanced Classification Algorithm...${BASHRC_NC}"
    
    # PHASE 2: Advanced Classification with Machine Learning-Style Logic
    while IFS='|' read -r file features; do
        [[ ! -f "$file" ]] && continue
        
        local filename=$(basename "$file")
        
        # Run advanced classification pipeline
        local classification_result=$(classify_file_advanced "$file" "$features" "$user_model" "$intelligence_dir")
        
        # Parse classification result
        IFS='|' read -r category confidence reasoning subcategory <<< "$classification_result"
        
        # Apply dynamic confidence thresholds based on learning
        local min_confidence=$(get_dynamic_confidence_threshold "$category" "$user_model")
        
        if [[ $(echo "$confidence < $min_confidence" | bc -l) == "1" ]]; then
            # Low confidence - use ensemble voting
            category=$(ensemble_classification "$file" "$features" "$user_model" "$intelligence_dir")
            confidence=$(echo "$confidence * 0.8" | bc -l) # Reduce confidence for ensemble
        fi
        
        # PHASE 3: Contextual Intelligence & Project Detection
        local context_boost=$(analyze_contextual_intelligence "$file" "$target_dir" "$intelligence_dir")
        confidence=$(echo "$confidence + $context_boost" | bc -l)
        
        # Cap confidence at 99%
        confidence=$(echo "if($confidence > 99) 99 else $confidence" | bc -l)
        
        # PHASE 4: Smart Directory Creation with Hierarchical Organization
        local final_path=$(determine_smart_path "$target_dir" "$category" "$subcategory" "$file" "$features")
        
        # Execute move or log
        if [[ "$dry_run" == "false" ]]; then
            mkdir -p "$(dirname "$final_path")"
            
            # Intelligent conflict resolution
            final_path=$(resolve_naming_conflict_intelligent "$final_path" "$file" "$features")
            
            if [[ "$final_path" != "$file" ]]; then
                mv "$file" "$final_path" && {
                    ((files_processed++))
                    echo "mv|$file|$final_path|$category|$confidence|$reasoning|$subcategory" >> "$operations_log"
                    
                    # Log for machine learning
                    echo "$(date -Iseconds)|MOVE|$file|$final_path|$category|$confidence|$features" >> "$learning_log"
                }
            fi
        else
            ((files_processed++))
            local relative_path=$(echo "$final_path" | sed "s|^$target_dir/||")
            echo "WOULD MOVE: $filename â†’ $relative_path"
            echo "  Category: $category (${confidence}% confidence)"
            echo "  Reasoning: $reasoning"
            [[ -n "$subcategory" ]] && echo "  Subcategory: $subcategory"
        fi
        
        [[ "$verbose" == "true" ]] && {
            echo -e "ğŸ§  ${filename}:"
            echo -e "   ğŸ“‚ Category: $category/$subcategory"
            echo -e "   ğŸ¯ Confidence: ${confidence}%"
            echo -e "   ğŸ’­ Reasoning: $reasoning"
        }
        
    done < "$temp_features"
    
    # Update user model with new learnings
    update_user_model_from_session "$user_model" "$learning_log"
    
    rm -f "$temp_features"
    echo $files_processed
}

# Advanced Multi-Dimensional Feature Extraction
extract_advanced_features() {
    local file="$1"
    local intelligence_dir="$2"
    local filename=$(basename "$file")
    local features=""
    
    # Feature 1: File Extension Intelligence
    local extension="${filename##*.}"
    [[ "$extension" == "$filename" ]] && extension="none"
    extension=$(echo "$extension" | tr '[:upper:]' '[:lower:]')
    
    # Feature 2: File Size Intelligence
    local size=$(stat -c%s "$file" 2>/dev/null || echo "0")
    local size_class=""
    if [[ $size -lt 1024 ]]; then size_class="tiny"
    elif [[ $size -lt 102400 ]]; then size_class="small"
    elif [[ $size -lt 10485760 ]]; then size_class="medium"
    elif [[ $size -lt 104857600 ]]; then size_class="large"
    else size_class="huge"; fi
    
    # Feature 3: MIME Type Analysis
    local mime_type=$(file --mime-type "$file" 2>/dev/null | cut -d: -f2 | tr -d ' ')
    local mime_category=$(echo "$mime_type" | cut -d/ -f1)
    local mime_subtype=$(echo "$mime_type" | cut -d/ -f2)
    
    # Feature 4: Filename Pattern Analysis
    local name_pattern=""
    case "$filename" in
        *screenshot*|*Screen?Shot*|*capture*) name_pattern="screenshot" ;;
        *backup*|*bak|*.bak) name_pattern="backup" ;;
        *temp*|*tmp*|*.tmp) name_pattern="temporary" ;;
        *test*|*spec*) name_pattern="test" ;;
        *config*|*settings*|*.conf|*.cfg) name_pattern="config" ;;
        *readme*|*README*) name_pattern="documentation" ;;
        *license*|*LICENSE*) name_pattern="legal" ;;
        *[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]*) name_pattern="dated" ;;
        *v[0-9]*|*version*) name_pattern="versioned" ;;
        *draft*|*Draft*) name_pattern="draft" ;;
        *final*|*Final*) name_pattern="final" ;;
    esac
    
    # Feature 5: Content Intelligence (for text-based files)
    local content_features=""
    if [[ "$mime_category" == "text" ]] || [[ "$extension" =~ ^(md|txt|py|js|html|css|json|yaml|yml|xml|csv)$ ]]; then
        content_features=$(analyze_text_content "$file" "$intelligence_dir")
    elif [[ "$mime_category" == "image" ]]; then
        content_features=$(analyze_image_metadata "$file" "$intelligence_dir")
    fi
    
    # Feature 6: Creation/Modification Time Intelligence
    local mod_time=$(stat -c%Y "$file" 2>/dev/null || echo "0")
    local age_days=$(( ($(date +%s) - mod_time) / 86400 ))
    local age_class=""
    if [[ $age_days -lt 1 ]]; then age_class="today"
    elif [[ $age_days -lt 7 ]]; then age_class="this_week"
    elif [[ $age_days -lt 30 ]]; then age_class="this_month"
    elif [[ $age_days -lt 365 ]]; then age_class="this_year"
    else age_class="old"; fi
    
    # Feature 7: Directory Context
    local parent_dir=$(basename "$(dirname "$file")")
    local context_hint=""
    case "$parent_dir" in
        Downloads|downloads) context_hint="downloaded" ;;
        Desktop|desktop) context_hint="desktop" ;;
        Documents|documents) context_hint="documents" ;;
        Pictures|pictures|Images|images) context_hint="images" ;;
        Videos|videos) context_hint="videos" ;;
        Music|music|Audio|audio) context_hint="audio" ;;
        *project*|*Project*) context_hint="project" ;;
        *work*|*Work*) context_hint="work" ;;
    esac
    
    # Compile all features
    features="ext:$extension,size:$size_class,mime:$mime_category/$mime_subtype,pattern:$name_pattern,age:$age_class,context:$context_hint"
    [[ -n "$content_features" ]] && features="$features,$content_features"
    
    echo "$features"
}

# Advanced Text Content Analysis
analyze_text_content() {
    local file="$1"
    local intelligence_dir="$2"
    local content_features=""
    
    # Only analyze first 10KB to avoid performance issues
    local sample=$(head -c 10240 "$file" 2>/dev/null)
    [[ -z "$sample" ]] && return
    
    # Programming language detection
    if [[ "$sample" =~ (function|def|class|import|require|include) ]]; then
        if [[ "$sample" =~ (def |class |import |from ) ]]; then
            content_features="lang:python"
        elif [[ "$sample" =~ (function|const|let|var|import|require) ]]; then
            content_features="lang:javascript"
        elif [[ "$sample" =~ (public|private|class|import|package) ]]; then
            content_features="lang:java"
        elif [[ "$sample" =~ (#include|int main|std::) ]]; then
            content_features="lang:cpp"
        elif [[ "$sample" =~ (fn |struct |impl |use ) ]]; then
            content_features="lang:rust"
        else
            content_features="lang:code"
        fi
    fi
    
    # Documentation detection
    if [[ "$sample" =~ (# |## |### |TODO|FIXME|NOTE) ]]; then
        [[ -n "$content_features" ]] && content_features="$content_features,doc:markdown" || content_features="doc:markdown"
    fi
    
    # Configuration file detection
    if [[ "$sample" =~ (\[.*\]|.*=.*|.*:.*) ]] && [[ ! "$sample" =~ (function|def|class) ]]; then
        [[ -n "$content_features" ]] && content_features="$content_features,type:config" || content_features="type:config"
    fi
    
    # Data file detection
    if [[ "$sample" =~ (^.*,.*,.*$|^\{.*\}$|\[.*\]) ]]; then
        [[ -n "$content_features" ]] && content_features="$content_features,type:data" || content_features="type:data"
    fi
    
    echo "$content_features"
}

# Advanced Image Metadata Analysis
analyze_image_metadata() {
    local file="$1"
    local intelligence_dir="$2"
    local image_features=""
    
    if command -v identify >/dev/null 2>&1; then
        local image_info=$(identify -format "%w %h %[EXIF:Make] %[EXIF:Software]" "$file" 2>/dev/null)
        local width height camera software
        read -r width height camera software <<< "$image_info"
        
        # Resolution-based classification
        if [[ -n "$width" && -n "$height" ]]; then
            local pixels=$((width * height))
            if [[ $pixels -lt 100000 ]]; then
                image_features="res:thumbnail"
            elif [[ $width -gt $height && $width -gt 1920 ]]; then
                image_features="res:wide"
            elif [[ $height -gt $width ]]; then
                image_features="res:portrait"
            else
                image_features="res:standard"
            fi
        fi
        
        # Camera/software detection
        [[ -n "$camera" ]] && image_features="$image_features,camera:true"
        if [[ "$software" =~ (Photoshop|GIMP|Illustrator) ]]; then
            image_features="$image_features,edited:true"
        fi
    fi
    
    echo "$image_features"
}

# Revolutionary AI-Style Classification Algorithm
classify_file_advanced() {
    local file="$1"
    local features="$2"
    local user_model="$3"
    local intelligence_dir="$4"
    
    local filename=$(basename "$file")
    local category="misc"
    local confidence=50
    local reasoning="default classification"
    local subcategory=""
    
    # Parse features into associative array
    declare -A feature_map
    IFS=',' read -ra feature_array <<< "$features"
    for feature in "${feature_array[@]}"; do
        local key="${feature%:*}"
        local value="${feature#*:}"
        feature_map["$key"]="$value"
    done
    
    # CLASSIFICATION PIPELINE
    
    # Rule 1: Programming Language Intelligence
    if [[ -n "${feature_map[lang]}" ]]; then
        case "${feature_map[lang]}" in
            python) category="code"; subcategory="python"; confidence=95; reasoning="Python code detected" ;;
            javascript) category="code"; subcategory="javascript"; confidence=95; reasoning="JavaScript code detected" ;;
            java) category="code"; subcategory="java"; confidence=95; reasoning="Java code detected" ;;
            cpp) category="code"; subcategory="cpp"; confidence=95; reasoning="C++ code detected" ;;
            rust) category="code"; subcategory="rust"; confidence=95; reasoning="Rust code detected" ;;
            *) category="code"; subcategory="source"; confidence=90; reasoning="Source code detected" ;;
        esac
    
    # Rule 2: Document Intelligence
    elif [[ "${feature_map[mime]}" =~ ^application/(pdf|msword|vnd\..*) ]] || [[ "${feature_map[doc]}" == "markdown" ]]; then
        category="documents"
        confidence=90
        reasoning="Document format detected"
        
        case "${feature_map[pattern]}" in
            documentation) subcategory="docs"; confidence=95; reasoning="Documentation pattern detected" ;;
            legal) subcategory="legal"; confidence=95; reasoning="Legal document pattern" ;;
            draft) subcategory="drafts"; confidence=85; reasoning="Draft document pattern" ;;
            final) subcategory="final"; confidence=85; reasoning="Final document pattern" ;;
        esac
    
    # Rule 3: Image Intelligence
    elif [[ "${feature_map[mime]}" =~ ^image/ ]]; then
        category="images"
        confidence=95
        reasoning="Image file detected"
        
        case "${feature_map[res]}" in
            thumbnail) subcategory="thumbnails"; reasoning="Thumbnail resolution detected" ;;
            wide) subcategory="wide"; reasoning="Wide format image detected" ;;
            portrait) subcategory="portraits"; reasoning="Portrait orientation detected" ;;
        esac
        
        [[ "${feature_map[camera]}" == "true" ]] && { subcategory="photos"; reasoning="Camera photo detected"; }
        [[ "${feature_map[edited]}" == "true" ]] && { subcategory="edited"; reasoning="Edited image detected"; }
        [[ "${feature_map[pattern]}" == "screenshot" ]] && { subcategory="screenshots"; confidence=98; reasoning="Screenshot pattern detected"; }
    
    # Rule 4: Media Intelligence
    elif [[ "${feature_map[mime]}" =~ ^(audio|video)/ ]]; then
        [[ "${feature_map[mime]}" =~ ^audio/ ]] && category="audio" || category="videos"
        confidence=95
        reasoning="Media file detected"
        
        case "${feature_map[size]}" in
            small) subcategory="clips"; reasoning="Small media file - likely clip" ;;
            huge) subcategory="full"; reasoning="Large media file - full content" ;;
        esac
    
    # Rule 5: Archive Intelligence
    elif [[ "${feature_map[ext]}" =~ ^(zip|tar|gz|bz2|xz|7z|rar)$ ]]; then
        category="archives"
        confidence=95
        reasoning="Archive format detected"
        
        [[ "${feature_map[pattern]}" == "backup" ]] && { subcategory="backups"; confidence=98; reasoning="Backup archive detected"; }
    
    # Rule 6: Configuration Intelligence
    elif [[ "${feature_map[type]}" == "config" ]] || [[ "${feature_map[pattern]}" == "config" ]]; then
        category="config"
        confidence=90
        reasoning="Configuration file detected"
    
    # Rule 7: Data Intelligence
    elif [[ "${feature_map[type]}" == "data" ]] || [[ "${feature_map[ext]}" =~ ^(json|csv|xml|yaml|yml|sql)$ ]]; then
        category="data"
        confidence=90
        reasoning="Data file format detected"
        
        case "${feature_map[ext]}" in
            json) subcategory="json"; reasoning="JSON data format" ;;
            csv) subcategory="spreadsheets"; reasoning="CSV spreadsheet data" ;;
            sql) subcategory="databases"; reasoning="SQL database file" ;;
        esac
    
    # Rule 8: Temporary/System File Intelligence
    elif [[ "${feature_map[pattern]}" =~ ^(temporary|backup)$ ]]; then
        category="system"
        case "${feature_map[pattern]}" in
            temporary) subcategory="temp"; confidence=95; reasoning="Temporary file pattern" ;;
            backup) subcategory="backups"; confidence=95; reasoning="Backup file pattern" ;;
        esac
    
    # Rule 9: Context-based Intelligence
    elif [[ -n "${feature_map[context]}" ]]; then
        case "${feature_map[context]}" in
            downloaded) category="downloads"; confidence=70; reasoning="Found in downloads context" ;;
            project) category="projects"; confidence=75; reasoning="Project context detected" ;;
            work) category="work"; confidence=75; reasoning="Work context detected" ;;
        esac
    fi
    
    # Apply user learning adjustments
    local user_adjustment=$(get_user_preference_adjustment "$filename" "$category" "$user_model")
    confidence=$(echo "$confidence + $user_adjustment" | bc -l)
    
    # Ensure confidence bounds
    confidence=$(echo "if($confidence > 99) 99 else if($confidence < 1) 1 else $confidence" | bc -l)
    
    printf "%.0f" "$confidence" | xargs printf "%s|%.0f|%s|%s\n" "$category" "$confidence" "$reasoning" "$subcategory"
}

# Load or create user intelligence model
load_or_create_user_model() {
    local user_model="$1"
    
    if [[ ! -f "$user_model" ]]; then
        cat > "$user_model" << 'EOF'
{
  "version": "1.0",
  "created": "",
  "last_updated": "",
  "user_preferences": {},
  "success_patterns": {},
  "confidence_thresholds": {
    "code": 85,
    "documents": 80,
    "images": 90,
    "videos": 90,
    "audio": 90,
    "archives": 95,
    "data": 85,
    "misc": 60
  },
  "learning_stats": {
    "total_classifications": 0,
    "successful_classifications": 0,
    "user_corrections": 0
  }
}
EOF
        sed -i "s/\"created\": \"\"/\"created\": \"$(date -Iseconds)\"/" "$user_model"
        sed -i "s/\"last_updated\": \"\"/\"last_updated\": \"$(date -Iseconds)\"/" "$user_model"
    fi
}

# Advanced Ensemble Classification for Low-Confidence Cases
ensemble_classification() {
    local file="$1"
    local features="$2"
    local user_model="$3"
    local intelligence_dir="$4"
    
    local filename=$(basename "$file")
    declare -A votes
    
    # Classifier 1: Extension-based with weighted confidence
    local ext="${filename##*.}"
    ext=$(echo "$ext" | tr '[:upper:]' '[:lower:]')
    
    case "$ext" in
        py|pyx|pyi) votes["code"]=$((${votes["code"]:-0} + 3)) ;;
        js|jsx|ts|tsx) votes["code"]=$((${votes["code"]:-0} + 3)) ;;
        html|css|scss|less) votes["web"]=$((${votes["web"]:-0} + 3)) ;;
        pdf|doc|docx|odt) votes["documents"]=$((${votes["documents"]:-0} + 3)) ;;
        jpg|jpeg|png|gif|bmp|tiff) votes["images"]=$((${votes["images"]:-0} + 3)) ;;
        mp4|avi|mkv|mov) votes["videos"]=$((${votes["videos"]:-0} + 3)) ;;
        mp3|wav|flac|ogg) votes["audio"]=$((${votes["audio"]:-0} + 3)) ;;
        zip|tar|gz|7z|rar) votes["archives"]=$((${votes["archives"]:-0} + 3)) ;;
    esac
    
    # Classifier 2: Content-based MIME type analysis
    local mime_type=$(file --mime-type "$file" 2>/dev/null | cut -d: -f2 | tr -d ' ')
    case "$mime_type" in
        text/*) votes["documents"]=$((${votes["documents"]:-0} + 2)) ;;
        image/*) votes["images"]=$((${votes["images"]:-0} + 2)) ;;
        video/*) votes["videos"]=$((${votes["videos"]:-0} + 2)) ;;
        audio/*) votes["audio"]=$((${votes["audio"]:-0} + 2)) ;;
        application/zip|*tar*|*rar*) votes["archives"]=$((${votes["archives"]:-0} + 2)) ;;
        application/pdf) votes["documents"]=$((${votes["documents"]:-0} + 2)) ;;
    esac
    
    # Classifier 3: Filename pattern analysis
    case "$filename" in
        *screenshot*|*Screenshot*) votes["images"]=$((${votes["images"]:-0} + 2)) ;;
        *backup*|*.bak) votes["archives"]=$((${votes["archives"]:-0} + 2)) ;;
        *config*|*.conf|*.cfg) votes["config"]=$((${votes["config"]:-0} + 2)) ;;
        *test*|*spec*) votes["code"]=$((${votes["code"]:-0} + 1)) ;;
        README*|*.md) votes["documents"]=$((${votes["documents"]:-0} + 1)) ;;
    esac
    
    # Classifier 4: Size-based intelligence
    local size=$(stat -c%s "$file" 2>/dev/null || echo "0")
    if [[ $size -gt 104857600 ]]; then # > 100MB
        [[ "$mime_type" =~ ^video/ ]] && votes["videos"]=$((${votes["videos"]:-0} + 1))
        [[ "$mime_type" =~ ^application/(zip|x-tar) ]] && votes["archives"]=$((${votes["archives"]:-0} + 1))
    fi
    
    # Find winner
    local max_votes=0
    local winner="misc"
    for category in "${!votes[@]}"; do
        if [[ ${votes[$category]} -gt $max_votes ]]; then
            max_votes=${votes[$category]}
            winner="$category"
        fi
    done
    
    echo "$winner"
}

# Contextual Intelligence Analysis
analyze_contextual_intelligence() {
    local file="$1"
    local target_dir="$2"
    local intelligence_dir="$3"
    local filename=$(basename "$file")
    local boost=0
    
    # Project context detection
    local project_indicators=0
    [[ -f "$target_dir/package.json" ]] && ((project_indicators++))
    [[ -f "$target_dir/requirements.txt" ]] && ((project_indicators++))
    [[ -f "$target_dir/Cargo.toml" ]] && ((project_indicators++))
    [[ -f "$target_dir/pom.xml" ]] && ((project_indicators++))
    [[ -f "$target_dir/Makefile" ]] && ((project_indicators++))
    [[ -d "$target_dir/.git" ]] && ((project_indicators++))
    
    # Boost confidence if in project context
    if [[ $project_indicators -gt 2 ]]; then
        case "$filename" in
            *.py|*.js|*.java|*.rs|*.cpp|*.c|*.h) boost=$(echo "$boost + 10" | bc -l) ;;
            *test*|*spec*) boost=$(echo "$boost + 5" | bc -l) ;;
            README*|*.md|docs/*) boost=$(echo "$boost + 5" | bc -l) ;;
        esac
    fi
    
    # Temporal context - recent files in same category
    local file_age=$(stat -c%Y "$file" 2>/dev/null || echo "0")
    local recent_threshold=$(($(date +%s) - 3600)) # 1 hour ago
    
    if [[ $file_age -gt $recent_threshold ]]; then
        boost=$(echo "$boost + 2" | bc -l)
    fi
    
    # Directory context patterns
    local parent_dir=$(basename "$(dirname "$file")")
    case "$parent_dir" in
        Downloads|downloads) boost=$(echo "$boost + 5" | bc -l) ;;
        Documents|documents) boost=$(echo "$boost + 3" | bc -l) ;;
        Pictures|Images) boost=$(echo "$boost + 3" | bc -l) ;;
    esac
    
    echo "$boost"
}

# Smart Path Determination with Hierarchical Organization
determine_smart_path() {
    local target_dir="$1"
    local category="$2"
    local subcategory="$3"
    local file="$4"
    local features="$5"
    local filename=$(basename "$file")
    
    local base_path="$target_dir/$category"
    
    # Create hierarchical structure based on category
    case "$category" in
        code)
            [[ -n "$subcategory" ]] && base_path="$base_path/$subcategory"
            ;;
        documents)
            case "$subcategory" in
                docs) base_path="$base_path/documentation" ;;
                legal) base_path="$base_path/legal" ;;
                drafts) base_path="$base_path/drafts" ;;
                final) base_path="$base_path/final" ;;
                *) base_path="$base_path/general" ;;
            esac
            ;;
        images)
            case "$subcategory" in
                screenshots) base_path="$base_path/screenshots" ;;
                photos) base_path="$base_path/photos" ;;
                edited) base_path="$base_path/edited" ;;
                thumbnails) base_path="$base_path/thumbnails" ;;
                *) base_path="$base_path/general" ;;
            esac
            ;;
        archives)
            [[ "$subcategory" == "backups" ]] && base_path="$base_path/backups" || base_path="$base_path/general"
            ;;
        data)
            case "$subcategory" in
                json) base_path="$base_path/json" ;;
                spreadsheets) base_path="$base_path/spreadsheets" ;;
                databases) base_path="$base_path/databases" ;;
                *) base_path="$base_path/general" ;;
            esac
            ;;
    esac
    
    echo "$base_path/$filename"
}

# Intelligent Conflict Resolution
resolve_naming_conflict_intelligent() {
    local target_path="$1"
    local original_file="$2"
    local features="$3"
    
    if [[ ! -e "$target_path" || "$target_path" == "$original_file" ]]; then
        echo "$target_path"
        return
    fi
    
    local dir=$(dirname "$target_path")
    local filename=$(basename "$target_path")
    local base_name="${filename%.*}"
    local extension="${filename##*.}"
    [[ "$extension" == "$filename" ]] && extension=""
    
    # Intelligent naming based on features
    local suffix=""
    
    # Check if files are actually different
    if cmp -s "$original_file" "$target_path" 2>/dev/null; then
        # Files are identical - don't move
        echo "$original_file"
        return
    fi
    
    # Parse features for intelligent suffix
    if [[ "$features" =~ age:today ]]; then
        suffix="_$(date +%H%M)"
    elif [[ "$features" =~ pattern:backup ]]; then
        suffix="_backup"
    elif [[ "$features" =~ pattern:draft ]]; then
        suffix="_draft"
    else
        # Default numbered suffix
        local counter=1
        while [[ -e "$dir/${base_name}_${counter}${extension:+.$extension}" ]]; do
            ((counter++))
        done
        suffix="_$counter"
    fi
    
    echo "$dir/${base_name}${suffix}${extension:+.$extension}"
}

# Get Dynamic Confidence Threshold
get_dynamic_confidence_threshold() {
    local category="$1"
    local user_model="$2"
    local default_threshold=70
    
    # This would normally parse JSON, but for simplicity:
    case "$category" in
        code) echo "85" ;;
        documents) echo "80" ;;
        images) echo "90" ;;
        videos|audio) echo "90" ;;
        archives) echo "95" ;;
        data) echo "85" ;;
        *) echo "$default_threshold" ;;
    esac
}

# Get User Preference Adjustment
get_user_preference_adjustment() {
    local filename="$1"
    local category="$2"
    local user_model="$3"
    
    # This would analyze user correction patterns from the model
    # For now, simple heuristics
    local adjustment=0
    
    # If user has previously corrected similar files, adjust confidence
    # This is simplified - real implementation would parse JSON model
    
    echo "$adjustment"
}

# Update User Model from Session Learning
update_user_model_from_session() {
    local user_model="$1"
    local learning_log="$2"
    
    [[ ! -f "$learning_log" ]] && return
    
    # Update the last_updated timestamp
    if command -v sed >/dev/null 2>&1; then
        sed -i "s/\"last_updated\": \".*\"/\"last_updated\": \"$(date -Iseconds)\"/" "$user_model"
    fi
    
    # This would update the JSON model with new patterns
    # Simplified implementation for now
    local session_classifications=$(wc -l < "$learning_log")
    echo "Session completed: $session_classifications classifications logged for learning"
}

# Legacy function for backward compatibility
detect_file_category_by_content() {
    local file="$1"
    local mime_type=$(file --mime-type "$file" 2>/dev/null | cut -d: -f2 | tr -d ' ')
    
    case "$mime_type" in
        text/*)                     echo "documents" ;;
        image/*)                    echo "images" ;;
        video/*)                    echo "videos" ;;
        audio/*)                    echo "audio" ;;
        application/zip|application/x-*tar*|application/x-*rar*) echo "archives" ;;
        application/pdf)            echo "documents" ;;
        application/msword|application/vnd.*) echo "documents" ;;
        application/x-executable)   echo "executables" ;;
        application/json|application/xml) echo "data" ;;
        *)                          echo "misc" ;;
    esac
}

# Organization by file type
organize_by_type() {
    local target_dir="$1"
    local recursive="$2"
    local dry_run="$3"
    local verbose="$4"
    local operations_log="$5"
    
    local files_processed=0
    local find_opts=(-maxdepth 1)
    [[ "$recursive" == "true" ]] && find_opts=()
    
    while IFS= read -r -d '' file; do
        [[ ! -f "$file" ]] && continue
        
        local filename=$(basename "$file")
        local extension="${filename##*.}"
        [[ "$extension" == "$filename" ]] && extension="no-extension"
        extension=$(echo "$extension" | tr '[:upper:]' '[:lower:]')
        
        local type_dir="$target_dir/by-type/$extension"
        
        if [[ "$dry_run" == "false" ]]; then
            mkdir -p "$type_dir"
            mv "$file" "$type_dir/" && {
                ((files_processed++))
                echo "mv|$file|$type_dir/$filename|type|100" >> "$operations_log"
            }
        else
            ((files_processed++))
            echo "WOULD MOVE: $file â†’ $type_dir/"
        fi
        
        [[ "$verbose" == "true" ]] && echo "ğŸ“„ $filename â†’ by-type/$extension"
        
    done < <(find "$target_dir" "${find_opts[@]}" -type f -print0)
    
    echo $files_processed
}

# Organization by date
organize_by_date() {
    local target_dir="$1"
    local recursive="$2"
    local dry_run="$3"
    local verbose="$4"
    local operations_log="$5"
    
    local files_processed=0
    local find_opts=(-maxdepth 1)
    [[ "$recursive" == "true" ]] && find_opts=()
    
    while IFS= read -r -d '' file; do
        [[ ! -f "$file" ]] && continue
        
        local filename=$(basename "$file")
        local file_date=$(date -r "$file" +%Y/%m 2>/dev/null || echo "unknown")
        local date_dir="$target_dir/by-date/$file_date"
        
        if [[ "$dry_run" == "false" ]]; then
            mkdir -p "$date_dir"
            mv "$file" "$date_dir/" && {
                ((files_processed++))
                echo "mv|$file|$date_dir/$filename|date|100" >> "$operations_log"
            }
        else
            ((files_processed++))
            echo "WOULD MOVE: $file â†’ $date_dir/"
        fi
        
        [[ "$verbose" == "true" ]] && echo "ğŸ“… $filename â†’ by-date/$file_date"
        
    done < <(find "$target_dir" "${find_opts[@]}" -type f -print0)
    
    echo $files_processed
}

# Organization by size
organize_by_size() {
    local target_dir="$1"
    local recursive="$2"
    local dry_run="$3"
    local verbose="$4"
    local operations_log="$5"
    
    local files_processed=0
    local find_opts=(-maxdepth 1)
    [[ "$recursive" == "true" ]] && find_opts=()
    
    while IFS= read -r -d '' file; do
        [[ ! -f "$file" ]] && continue
        
        local filename=$(basename "$file")
        local file_size=$(stat -c%s "$file" 2>/dev/null || echo "0")
        local size_category=""
        
        if [[ $file_size -lt 1024 ]]; then
            size_category="tiny"
        elif [[ $file_size -lt 102400 ]]; then
            size_category="small"
        elif [[ $file_size -lt 10485760 ]]; then
            size_category="medium"
        elif [[ $file_size -lt 104857600 ]]; then
            size_category="large"
        else
            size_category="huge"
        fi
        
        local size_dir="$target_dir/by-size/$size_category"
        
        if [[ "$dry_run" == "false" ]]; then
            mkdir -p "$size_dir"
            mv "$file" "$size_dir/" && {
                ((files_processed++))
                echo "mv|$file|$size_dir/$filename|size|100" >> "$operations_log"
            }
        else
            ((files_processed++))
            echo "WOULD MOVE: $file â†’ $size_dir/"
        fi
        
        [[ "$verbose" == "true" ]] && echo "ğŸ“ $filename â†’ by-size/$size_category"
        
    done < <(find "$target_dir" "${find_opts[@]}" -type f -print0)
    
    echo $files_processed
}

# Organization by project structure
organize_by_project() {
    local target_dir="$1"
    local recursive="$2"
    local dry_run="$3"
    local verbose="$4"
    local operations_log="$5"
    
    local files_processed=0
    
    # Create standard project structure
    local project_dirs=("src" "docs" "tests" "config" "assets" "scripts" "build" "misc")
    
    for dir in "${project_dirs[@]}"; do
        [[ "$dry_run" == "false" ]] && mkdir -p "$target_dir/$dir"
    done
    
    local find_opts=(-maxdepth 1)
    [[ "$recursive" == "true" ]] && find_opts=()
    
    while IFS= read -r -d '' file; do
        [[ ! -f "$file" ]] && continue
        
        local filename=$(basename "$file")
        local extension="${filename##*.}"
        extension=$(echo "$extension" | tr '[:upper:]' '[:lower:]')
        local project_category="misc"
        
        # Determine project category
        case "$extension" in
            py|js|java|cpp|c|h|go|rs|php|rb|kt) project_category="src" ;;
            md|txt|pdf|doc|docx) project_category="docs" ;;
            json|yaml|yml|xml|conf|cfg|ini) project_category="config" ;;
            png|jpg|jpeg|gif|svg|ico) project_category="assets" ;;
            sh|bat|ps1|cmd) project_category="scripts" ;;
            test.*)  project_category="tests" ;;
            *test*|*spec*) project_category="tests" ;;
            exe|bin|out|jar|war) project_category="build" ;;
        esac
        
        # Special filename patterns
        case "$filename" in
            *test*|*spec*) project_category="tests" ;;
            README*|CHANGELOG*|LICENSE*) project_category="docs" ;;
            Makefile|Dockerfile|docker-compose*) project_category="config" ;;
            build.*|dist.*) project_category="build" ;;
        esac
        
        local project_dir="$target_dir/$project_category"
        
        if [[ "$dry_run" == "false" ]]; then
            mv "$file" "$project_dir/" && {
                ((files_processed++))
                echo "mv|$file|$project_dir/$filename|project|100" >> "$operations_log"
            }
        else
            ((files_processed++))
            echo "WOULD MOVE: $file â†’ $project_category/"
        fi
        
        [[ "$verbose" == "true" ]] && echo "ğŸ—‚ï¸  $filename â†’ $project_category"
        
    done < <(find "$target_dir" "${find_opts[@]}" -type f -print0)
    
    echo $files_processed
}

# Undo last organization
undo_organization() {
    local target_dir="$1"
    local history_file="$2"
    
    [[ ! -f "$history_file" ]] && { echo "âŒ No organization history found"; return 1; }
    
    echo -e "ğŸ”„ ${BASHRC_CYAN}Searching for last organization...${BASHRC_NC}"
    
    # Find the last organization entry for this directory
    local last_operation=$(grep "$(realpath "$target_dir")" "$history_file" | tail -1)
    [[ -z "$last_operation" ]] && { echo "âŒ No organization found for this directory"; return 1; }
    
    echo -e "ğŸ“‹ ${BASHRC_YELLOW}Found operation:${BASHRC_NC} $last_operation"
    
    # Extract the operations that follow this entry
    local temp_undo=$(mktemp)
    local found_marker=false
    
    while IFS= read -r line; do
        if [[ "$line" == "$last_operation" ]]; then
            found_marker=true
            continue
        fi
        
        if [[ "$found_marker" == "true" ]]; then
            [[ "$line" == "---" ]] && break
            [[ "$line" =~ ^mv\| ]] && echo "$line" >> "$temp_undo"
        fi
    done < "$history_file"
    
    local undo_count=$(wc -l < "$temp_undo")
    [[ $undo_count -eq 0 ]] && { echo "âŒ No operations to undo"; rm -f "$temp_undo"; return 1; }
    
    echo -e "ğŸ”„ ${BASHRC_CYAN}Undoing $undo_count file operations...${BASHRC_NC}"
    
    # Process undo operations in reverse order
    local undone=0
    while IFS='|' read -r operation source target category confidence; do
        if [[ -f "$target" ]]; then
            local original_dir=$(dirname "$source")
            mkdir -p "$original_dir"
            mv "$target" "$source" && {
                ((undone++))
                echo "âœ… Restored: $target â†’ $source"
            }
        else
            echo "âš ï¸  File not found: $target"
        fi
    done < <(tac "$temp_undo")
    
    echo -e "âœ… ${BASHRC_GREEN}Undo completed: $undone files restored${BASHRC_NC}"
    rm -f "$temp_undo"
}

# Learn organization patterns
learn_organization_patterns() {
    local target_dir="$1"
    local patterns_file="$2"
    
    echo -e "ğŸ“ ${BASHRC_CYAN}Analyzing current organization patterns...${BASHRC_NC}"
    
    # Analyze current directory structure to learn patterns
    local temp_patterns=$(mktemp)
    
    # Look for organized subdirectories
    find "$target_dir" -maxdepth 2 -type d | while read -r dir; do
        local subdir=$(basename "$dir")
        [[ "$subdir" == "." || "$subdir" == "$(basename "$target_dir")" ]] && continue
        
        # Analyze files in this subdirectory
        local file_count=$(find "$dir" -maxdepth 1 -type f | wc -l)
        [[ $file_count -eq 0 ]] && continue
        
        # Get file type patterns
        local extensions=$(find "$dir" -maxdepth 1 -type f -name "*.*" | \
                         sed 's/.*\.//' | tr '[:upper:]' '[:lower:]' | \
                         sort | uniq -c | sort -rn | head -3)
        
        echo "Pattern detected: Directory '$subdir' contains files with extensions: $extensions"
    done > "$temp_patterns"
    
    # Store learned patterns (simplified - in real implementation, this would be more sophisticated)
    if [[ -s "$temp_patterns" ]]; then
        echo -e "ğŸ’¾ ${BASHRC_GREEN}Learned $(wc -l < "$temp_patterns") organization patterns${BASHRC_NC}"
        cat "$temp_patterns" >> "$patterns_file"
    fi
    
    rm -f "$temp_patterns"
}

# =============================================================================
# INTELLIGENT DUPLICATE DETECTION AND MANAGEMENT
# =============================================================================

# Advanced duplicate finder with multiple detection algorithms
finddups() {
    local usage="Usage: finddups [OPTIONS] [DIRECTORY...]
    
ğŸ” Intelligent Duplicate File Detection
Options:
    -a, --algorithm ALGO    Detection algorithm (hash, size, name, fuzzy, content)
    -r, --recursive         Recursive search
    -s, --min-size SIZE     Minimum file size (default: 1KB)
    -x, --exclude PATTERN   Exclude pattern
    -o, --output FORMAT     Output format (text, csv, json)
    -d, --delete            Delete duplicates interactively
    --auto-delete           Delete duplicates automatically (keep newest)
    --group-by GROUP        Group results by (directory, extension, size)
    -v, --verbose           Verbose output
    -h, --help              Show this help
    
Algorithms:
    hash     - Compare file hashes (most accurate)
    size     - Compare file sizes (fastest)
    name     - Compare filenames (fuzzy matching)
    fuzzy    - Fuzzy content comparison
    content  - Compare file content directly
    
Examples:
    finddups ~/Downloads                    # Find duplicates in Downloads
    finddups -a hash -s 10MB --auto-delete # Auto-delete hash duplicates over 10MB
    finddups -a fuzzy -o json               # Fuzzy matching with JSON output"

    local directories=()
    local algorithm="hash"
    local recursive_mode=false
    local min_size="1024"  # 1KB default
    local exclude_patterns=()
    local output_format="text"
    local interactive_delete=false
    local auto_delete=false
    local group_by=""
    local verbose_mode=false
    
    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -a|--algorithm)     algorithm="$2"; shift 2 ;;
            -r|--recursive)     recursive_mode=true; shift ;;
            -s|--min-size)      min_size=$(parse_size "$2"); shift 2 ;;
            -x|--exclude)       exclude_patterns+=("$2"); shift 2 ;;
            -o|--output)        output_format="$2"; shift 2 ;;
            -d|--delete)        interactive_delete=true; shift ;;
            --auto-delete)      auto_delete=true; shift ;;
            --group-by)         group_by="$2"; shift 2 ;;
            -v|--verbose)       verbose_mode=true; shift ;;
            -h|--help)          echo "$usage"; return 0 ;;
            -*)                 echo "Unknown option: $1" >&2; return 1 ;;
            *)                  directories+=("$1"); shift ;;
        esac
    done
    
    # Default to current directory if none specified
    [[ ${#directories[@]} -eq 0 ]] && directories=(".")
    
    # Validate directories
    for dir in "${directories[@]}"; do
        [[ ! -d "$dir" ]] && { echo "âŒ Directory not found: $dir"; return 1; }
    done
    
    echo -e "ğŸ” ${BASHRC_CYAN}Intelligent Duplicate Detection Starting...${BASHRC_NC}"
    echo -e "ğŸ“ ${BASHRC_YELLOW}Directories:${BASHRC_NC} ${directories[*]}"
    echo -e "ğŸ§® ${BASHRC_YELLOW}Algorithm:${BASHRC_NC} $algorithm"
    echo -e "ğŸ“ ${BASHRC_YELLOW}Min Size:${BASHRC_NC} $(numfmt --to=iec-i --suffix=B "$min_size")"
    
    local start_time=$(date +%s)
    
    # Build find command
    local find_cmd=(find "${directories[@]}")
    [[ "$recursive_mode" == "false" ]] && find_cmd+=(-maxdepth 1)
    find_cmd+=(-type f -size "+${min_size}c")
    
    # Add exclude patterns
    for pattern in "${exclude_patterns[@]}"; do
        find_cmd+=(-not -name "$pattern")
    done
    
    # Collect files
    echo -e "ğŸ“Š ${BASHRC_CYAN}Scanning for files...${BASHRC_NC}"
    local temp_filelist=$(mktemp)
    "${find_cmd[@]}" > "$temp_filelist"
    
    local total_files=$(wc -l < "$temp_filelist")
    [[ $total_files -eq 0 ]] && { echo "âŒ No files found matching criteria"; return 1; }
    
    echo -e "âœ… Found $total_files files to analyze"
    
    # Execute duplicate detection based on algorithm
    local duplicates_found=0
    local temp_results=$(mktemp)
    
    case "$algorithm" in
        hash)
            duplicates_found=$(find_duplicates_by_hash "$temp_filelist" "$temp_results" "$verbose_mode")
            ;;
        size)
            duplicates_found=$(find_duplicates_by_size "$temp_filelist" "$temp_results" "$verbose_mode")
            ;;
        name)
            duplicates_found=$(find_duplicates_by_name "$temp_filelist" "$temp_results" "$verbose_mode")
            ;;
        fuzzy)
            duplicates_found=$(find_duplicates_by_fuzzy "$temp_filelist" "$temp_results" "$verbose_mode")
            ;;
        content)
            duplicates_found=$(find_duplicates_by_content "$temp_filelist" "$temp_results" "$verbose_mode")
            ;;
        *)
            echo "âŒ Unknown algorithm: $algorithm"
            rm -f "$temp_filelist" "$temp_results"
            return 1
            ;;
    esac
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    echo -e "\nğŸ“Š ${BASHRC_PURPLE}Duplicate Detection Results${BASHRC_NC}"
    echo -e "${BASHRC_CYAN}$(printf '=%.0s' {1..40})${BASHRC_NC}"
    echo -e "ğŸ” Files analyzed: $total_files"
    echo -e "ğŸ”„ Duplicates found: $duplicates_found"
    echo -e "â±ï¸  Analysis time: ${duration}s"
    
    # Process results based on output format
    if [[ $duplicates_found -gt 0 ]]; then
        case "$output_format" in
            text)
                display_duplicates_text "$temp_results" "$group_by"
                ;;
            csv)
                display_duplicates_csv "$temp_results"
                ;;
            json)
                display_duplicates_json "$temp_results"
                ;;
        esac
        
        # Handle deletion options
        if [[ "$interactive_delete" == "true" ]]; then
            interactive_duplicate_deletion "$temp_results"
        elif [[ "$auto_delete" == "true" ]]; then
            automatic_duplicate_deletion "$temp_results"
        fi
    else
        echo -e "âœ… ${BASHRC_GREEN}No duplicates found!${BASHRC_NC}"
    fi
    
    rm -f "$temp_filelist" "$temp_results"
}

# Helper function to parse size strings
parse_size() {
    local size_str="$1"
    local size_bytes=""
    
    case "$size_str" in
        *B|*b)   size_bytes="${size_str%[Bb]}" ;;
        *K|*k)   size_bytes=$(echo "${size_str%[Kk]} * 1024" | bc) ;;
        *M|*m)   size_bytes=$(echo "${size_str%[Mm]} * 1024 * 1024" | bc) ;;
        *G|*g)   size_bytes=$(echo "${size_str%[Gg]} * 1024 * 1024 * 1024" | bc) ;;
        *)       size_bytes="$size_str" ;;
    esac
    
    echo "$size_bytes"
}

# Find duplicates by hash
find_duplicates_by_hash() {
    local filelist="$1"
    local results="$2"
    local verbose="$3"
    
    echo -e "ğŸ” ${BASHRC_CYAN}Computing file hashes...${BASHRC_NC}"
    
    local temp_hashes=$(mktemp)
    local duplicates=0
    
    # Compute hashes with progress
    local count=0
    local total=$(wc -l < "$filelist")
    
    while IFS= read -r file; do
        [[ ! -f "$file" ]] && continue
        
        ((count++))
        [[ "$verbose" == "true" ]] && echo -ne "\rğŸ”„ Progress: $count/$total"
        
        local hash=$(md5sum "$file" 2>/dev/null | cut -d' ' -f1)
        [[ -n "$hash" ]] && echo "$hash|$file" >> "$temp_hashes"
    done < "$filelist"
    
    [[ "$verbose" == "true" ]] && echo
    
    # Find duplicates
    sort "$temp_hashes" | cut -d'|' -f1 | uniq -d | while read -r dup_hash; do
        echo "DUPLICATE_GROUP:hash:$dup_hash" >> "$results"
        grep "^$dup_hash|" "$temp_hashes" | cut -d'|' -f2 | while read -r dup_file; do
            echo "  FILE:$dup_file" >> "$results"
            ((duplicates++))
        done
    done
    
    rm -f "$temp_hashes"
    echo "$duplicates"
}

# Find duplicates by size
find_duplicates_by_size() {
    local filelist="$1"
    local results="$2"
    local verbose="$3"
    
    echo -e "ğŸ“ ${BASHRC_CYAN}Analyzing file sizes...${BASHRC_NC}"
    
    local temp_sizes=$(mktemp)
    local duplicates=0
    
    # Get file sizes
    while IFS= read -r file; do
        [[ ! -f "$file" ]] && continue
        local size=$(stat -c%s "$file" 2>/dev/null)
        [[ -n "$size" ]] && echo "$size|$file" >> "$temp_sizes"
    done < "$filelist"
    
    # Find size duplicates
    sort "$temp_sizes" | cut -d'|' -f1 | uniq -d | while read -r dup_size; do
        local size_human=$(numfmt --to=iec-i --suffix=B "$dup_size")
        echo "DUPLICATE_GROUP:size:$dup_size ($size_human)" >> "$results"
        grep "^$dup_size|" "$temp_sizes" | cut -d'|' -f2 | while read -r dup_file; do
            echo "  FILE:$dup_file" >> "$results"
            ((duplicates++))
        done
    done
    
    rm -f "$temp_sizes"
    echo "$duplicates"
}

# Display duplicates in text format
display_duplicates_text() {
    local results_file="$1"
    local group_by="$2"
    
    echo -e "\nğŸ“‹ ${BASHRC_PURPLE}Duplicate Groups:${BASHRC_NC}"
    echo -e "${BASHRC_CYAN}$(printf '=%.0s' {1..50})${BASHRC_NC}\n"
    
    local group_count=0
    while IFS= read -r line; do
        if [[ "$line" =~ ^DUPLICATE_GROUP: ]]; then
            ((group_count++))
            local group_info="${line#DUPLICATE_GROUP:}"
            echo -e "${BASHRC_YELLOW}Group $group_count:${BASHRC_NC} $group_info"
        elif [[ "$line" =~ ^[[:space:]]+FILE: ]]; then
            local file="${line#  FILE:}"
            local file_size=$(stat -c%s "$file" 2>/dev/null || echo "0")
            local file_size_human=$(numfmt --to=iec-i --suffix=B "$file_size" 2>/dev/null || echo "?")
            local file_date=$(date -r "$file" +"%Y-%m-%d %H:%M" 2>/dev/null || echo "unknown")
            printf "   ğŸ“„ %s\n      ğŸ“ %s | ğŸ“… %s\n" "$(basename "$file")" "$file_size_human" "$file_date"
            printf "      ğŸ“ %s\n" "$(dirname "$file")"
        fi
        echo
    done < "$results_file"
}

# Interactive duplicate deletion
interactive_duplicate_deletion() {
    local results_file="$1"
    
    echo -e "\nğŸ—‘ï¸  ${BASHRC_CYAN}Interactive Duplicate Deletion${BASHRC_NC}"
    echo -e "${BASHRC_YELLOW}For each duplicate group, select files to delete:${BASHRC_NC}\n"
    
    local group_files=()
    local in_group=false
    local group_info=""
    
    while IFS= read -r line; do
        if [[ "$line" =~ ^DUPLICATE_GROUP: ]]; then
            # Process previous group if it exists
            if [[ "$in_group" == "true" && ${#group_files[@]} -gt 1 ]]; then
                process_duplicate_group_interactive "$group_info" "${group_files[@]}"
            fi
            
            # Start new group
            group_info="${line#DUPLICATE_GROUP:}"
            group_files=()
            in_group=true
        elif [[ "$line" =~ ^[[:space:]]+FILE: ]]; then
            group_files+=("${line#  FILE:}")
        fi
    done < "$results_file"
    
    # Process last group
    if [[ "$in_group" == "true" && ${#group_files[@]} -gt 1 ]]; then
        process_duplicate_group_interactive "$group_info" "${group_files[@]}"
    fi
}

# Process duplicate group interactively
process_duplicate_group_interactive() {
    local group_info="$1"
    shift
    local files=("$@")
    
    echo -e "${BASHRC_PURPLE}Duplicate Group:${BASHRC_NC} $group_info"
    echo -e "${BASHRC_CYAN}Files in group:${BASHRC_NC}"
    
    local i=1
    for file in "${files[@]}"; do
        local file_size=$(stat -c%s "$file" 2>/dev/null || echo "0")
        local file_size_human=$(numfmt --to=iec-i --suffix=B "$file_size" 2>/dev/null || echo "?")
        local file_date=$(date -r "$file" +"%Y-%m-%d %H:%M" 2>/dev/null || echo "unknown")
        printf "%2d. %s\n    ğŸ“ %s | ğŸ“… %s\n    ğŸ“ %s\n" "$i" "$(basename "$file")" "$file_size_human" "$file_date" "$(dirname "$file")"
        ((i++))
    done
    
    echo -e "\n${BASHRC_YELLOW}Options:${BASHRC_NC}"
    echo "   Numbers (1-$((${#files[@]})): Delete specific files"
    echo "   'k N': Keep only file N, delete others"
    echo "   'a': Delete all but newest"
    echo "   's': Skip this group"
    echo "   'q': Quit"
    
    read -p "Your choice: " -r choice
    
    case "$choice" in
        k\ [0-9]*)
            local keep_index="${choice#k }"
            if [[ $keep_index -ge 1 && $keep_index -le ${#files[@]} ]]; then
                for ((j=1; j<=${#files[@]}; j++)); do
                    [[ $j -ne $keep_index ]] && {
                        echo "ğŸ—‘ï¸  Deleting: ${files[$((j-1))]}"
                        rm -f "${files[$((j-1))]}"
                    }
                done
            fi
            ;;
        a)
            # Keep newest, delete others
            local newest_file=""
            local newest_time=0
            for file in "${files[@]}"; do
                local file_time=$(stat -c%Y "$file" 2>/dev/null || echo "0")
                if [[ $file_time -gt $newest_time ]]; then
                    newest_time=$file_time
                    newest_file="$file"
                fi
            done
            
            for file in "${files[@]}"; do
                [[ "$file" != "$newest_file" ]] && {
                    echo "ğŸ—‘ï¸  Deleting: $file"
                    rm -f "$file"
                }
            done
            echo "âœ… Kept newest: $newest_file"
            ;;
        [0-9]*)
            # Delete specific files
            IFS=' ' read -ra delete_indices <<< "$choice"
            for index in "${delete_indices[@]}"; do
                if [[ $index -ge 1 && $index -le ${#files[@]} ]]; then
                    echo "ğŸ—‘ï¸  Deleting: ${files[$((index-1))]}"
                    rm -f "${files[$((index-1))]}"
                fi
            done
            ;;
        q)
            echo "ğŸ‘‹ Quitting duplicate deletion"
            return 1
            ;;
        s)
            echo "â­ï¸  Skipping group"
            ;;
        *)
            echo "â“ Unknown option, skipping group"
            ;;
    esac
    
    echo
}

# =============================================================================
# REAL-TIME FILE MONITORING SYSTEM
# =============================================================================

# Intelligent file watcher with pattern recognition
watchfiles() {
    local usage="Usage: watchfiles [OPTIONS] PATH [PATH...]
    
ğŸ‘ï¸  Intelligent File Monitoring System
Options:
    -e, --events EVENT      Monitor specific events (create,modify,delete,move)
    -p, --pattern PATTERN   Only monitor files matching pattern
    -x, --exclude PATTERN   Exclude files matching pattern
    -r, --recursive         Monitor recursively
    -a, --action COMMAND    Execute command on event (\$FILE, \$EVENT available)
    -l, --log FILE          Log events to file
    -q, --quiet             Suppress console output
    -s, --stats             Show monitoring statistics
    --alert EMAIL           Send email alerts (requires mail command)
    --webhook URL           Send webhook notifications
    -h, --help              Show this help
    
Events: create, modify, delete, move, attrib
    
Examples:
    watchfiles ~/Documents                        # Monitor Documents folder
    watchfiles -r -p '*.py' --action 'echo Py file changed: \$FILE' src/
    watchfiles -e modify -l /var/log/changes.log /etc/
    watchfiles --alert admin@company.com --webhook http://api.company.com/notify /var/www/"

    local watch_paths=()
    local monitor_events="create,modify,delete,move"
    local file_pattern=""
    local exclude_patterns=()
    local recursive_mode=false
    local action_command=""
    local log_file=""
    local quiet_mode=false
    local show_stats=false
    local alert_email=""
    local webhook_url=""
    
    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -e|--events)        monitor_events="$2"; shift 2 ;;
            -p|--pattern)       file_pattern="$2"; shift 2 ;;
            -x|--exclude)       exclude_patterns+=("$2"); shift 2 ;;
            -r|--recursive)     recursive_mode=true; shift ;;
            -a|--action)        action_command="$2"; shift 2 ;;
            -l|--log)           log_file="$2"; shift 2 ;;
            -q|--quiet)         quiet_mode=true; shift ;;
            -s|--stats)         show_stats=true; shift ;;
            --alert)            alert_email="$2"; shift 2 ;;
            --webhook)          webhook_url="$2"; shift 2 ;;
            -h|--help)          echo "$usage"; return 0 ;;
            -*)                 echo "Unknown option: $1" >&2; return 1 ;;
            *)                  watch_paths+=("$1"); shift ;;
        esac
    done
    
    [[ ${#watch_paths[@]} -eq 0 ]] && { echo "$usage"; return 1; }
    
    # Validate paths
    for path in "${watch_paths[@]}"; do
        [[ ! -e "$path" ]] && { echo "âŒ Path not found: $path"; return 1; }
    done
    
    # Check for inotifywait
    if ! command -v inotifywait >/dev/null 2>&1; then
        echo "âŒ inotifywait not installed. Please install inotify-tools package."
        return 1
    fi
    
    echo -e "ğŸ‘ï¸  ${BASHRC_CYAN}Intelligent File Monitoring Starting...${BASHRC_NC}"
    echo -e "ğŸ“ ${BASHRC_YELLOW}Paths:${BASHRC_NC} ${watch_paths[*]}"
    echo -e "âš¡ ${BASHRC_YELLOW}Events:${BASHRC_NC} $monitor_events"
    [[ -n "$file_pattern" ]] && echo -e "ğŸ¯ ${BASHRC_YELLOW}Pattern:${BASHRC_NC} $file_pattern"
    [[ "$recursive_mode" == "true" ]] && echo -e "ğŸ”„ ${BASHRC_YELLOW}Recursive:${BASHRC_NC} Yes"
    
    # Set up monitoring statistics
    local monitor_dir="$HOME/.bash_file_operations/monitoring"
    mkdir -p "$monitor_dir"
    local stats_file="$monitor_dir/watch_stats_$(date +%Y%m%d_%H%M%S).json"
    local start_time=$(date +%s)
    
    # Initialize statistics
    cat > "$stats_file" << EOF
{
  "start_time": "$(date -Iseconds)",
  "paths": [$(printf '"%s",' "${watch_paths[@]}" | sed 's/,$//')],
  "events_monitored": "$monitor_events",
  "pattern": "$file_pattern",
  "recursive": $recursive_mode,
  "event_counts": {}
}
EOF
    
    # Build inotifywait command
    local inotify_opts=()
    [[ "$recursive_mode" == "true" ]] && inotify_opts+=(-r)
    [[ "$quiet_mode" == "true" ]] && inotify_opts+=(-q)
    inotify_opts+=(-m -e "$monitor_events" --format '%w%f|%e|%T' --timefmt '%Y-%m-%d %H:%M:%S')
    
    # Apply file pattern filter
    if [[ -n "$file_pattern" ]]; then
        inotify_opts+=(--include "$file_pattern")
    fi
    
    # Setup log file
    if [[ -n "$log_file" ]]; then
        touch "$log_file"
        echo "$(date -Iseconds) - File monitoring started for: ${watch_paths[*]}" >> "$log_file"
    fi
    
    echo -e "âœ… ${BASHRC_GREEN}Monitoring active - Press Ctrl+C to stop${BASHRC_NC}\n"
    
    # Trap cleanup
    trap 'cleanup_file_monitoring "$stats_file" "$log_file"' EXIT INT TERM
    
    # Start monitoring with intelligent processing
    inotifywait "${inotify_opts[@]}" "${watch_paths[@]}" | while IFS='|' read -r filepath event_type timestamp; do
        # Skip if file matches exclude patterns
        local should_exclude=false
        for exclude_pattern in "${exclude_patterns[@]}"; do
            if [[ "$filepath" =~ $exclude_pattern ]]; then
                should_exclude=true
                break
            fi
        done
        [[ "$should_exclude" == "true" ]] && continue
        
        # Process the event
        local filename=$(basename "$filepath")
        local directory=$(dirname "$filepath")
        
        # Update statistics (simplified)
        update_monitoring_stats "$stats_file" "$event_type"
        
        # Display event (unless quiet)
        if [[ "$quiet_mode" == "false" ]]; then
            local event_icon="ğŸ“"
            case "$event_type" in
                *CREATE*) event_icon="ğŸ†•" ;;
                *DELETE*) event_icon="ğŸ—‘ï¸" ;;
                *MODIFY*) event_icon="âœï¸" ;;
                *MOVED*) event_icon="ğŸ“¦" ;;
                *ATTRIB*) event_icon="âš™ï¸" ;;
            esac
            
            echo -e "$event_icon ${BASHRC_CYAN}[$timestamp]${BASHRC_NC} $event_type: $filepath"
        fi
        
        # Log to file if specified
        if [[ -n "$log_file" ]]; then
            echo "$timestamp|$event_type|$filepath" >> "$log_file"
        fi
        
        # Execute action command if specified
        if [[ -n "$action_command" ]]; then
            local expanded_command="${action_command//\$FILE/$filepath}"
            expanded_command="${expanded_command//\$EVENT/$event_type}"
            expanded_command="${expanded_command//\$TIMESTAMP/$timestamp}"
            
            eval "$expanded_command" &
        fi
        
        # Send notifications if configured
        if [[ -n "$alert_email" ]] && command -v mail >/dev/null 2>&1; then
            echo "File event detected: $event_type on $filepath at $timestamp" | \
                mail -s "File Monitor Alert" "$alert_email" &
        fi
        
        if [[ -n "$webhook_url" ]] && command -v curl >/dev/null 2>&1; then
            curl -X POST "$webhook_url" \
                -H "Content-Type: application/json" \
                -d "{\"event\":\"$event_type\",\"file\":\"$filepath\",\"timestamp\":\"$timestamp\"}" &
        fi
    done
}

# Update monitoring statistics
update_monitoring_stats() {
    local stats_file="$1"
    local event_type="$2"
    
    # This is a simplified version - in a real implementation, you'd use jq or a proper JSON parser
    echo "Event: $event_type" >> "${stats_file}.events"
}

# Cleanup function for file monitoring
cleanup_file_monitoring() {
    local stats_file="$1"
    local log_file="$2"
    
    echo -e "\nğŸ›‘ ${BASHRC_YELLOW}File monitoring stopped${BASHRC_NC}"
    
    if [[ -f "$stats_file" ]]; then
        local total_events=$(wc -l < "${stats_file}.events" 2>/dev/null || echo "0")
        local duration=$(( $(date +%s) - $(date -d "$(grep start_time "$stats_file" | cut -d'"' -f4)" +%s) ))
        echo -e "ğŸ“Š ${BASHRC_CYAN}Statistics:${BASHRC_NC}"
        echo -e "   Events captured: $total_events"
        echo -e "   Monitoring duration: ${duration}s"
        [[ -f "${stats_file}.events" ]] && rm -f "${stats_file}.events"
    fi
    
    [[ -n "$log_file" ]] && echo "$(date -Iseconds) - File monitoring stopped" >> "$log_file"
}

# =============================================================================
# ADVANCED FILE COMPARISON AND SYNCHRONIZATION
# =============================================================================

# Intelligent file comparison with semantic understanding
smartdiff() {
    local usage="Usage: smartdiff [OPTIONS] FILE1 FILE2
    
ğŸ” Intelligent File Comparison
Options:
    -t, --type TYPE         Comparison type (text, binary, semantic, structure)
    -i, --ignore-case       Ignore case differences
    -w, --ignore-whitespace Ignore whitespace changes
    -c, --context N         Show N lines of context (default: 3)
    -s, --side-by-side      Side-by-side comparison
    -n, --no-color          Disable colored output
    --similarity            Show similarity percentage
    --summary              Show summary only
    -h, --help              Show this help
    
Types:
    text      - Standard text diff with enhancements
    binary    - Binary file comparison
    semantic  - Semantic comparison (for code)
    structure - Compare file structure/organization
    
Features: Smart formatting, similarity analysis, semantic understanding"

    local file1=""
    local file2=""
    local comparison_type="text"
    local ignore_case=false
    local ignore_whitespace=false
    local context_lines=3
    local side_by_side=false
    local no_color=false
    local show_similarity=false
    local summary_only=false
    
    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -t|--type)              comparison_type="$2"; shift 2 ;;
            -i|--ignore-case)       ignore_case=true; shift ;;
            -w|--ignore-whitespace) ignore_whitespace=true; shift ;;
            -c|--context)           context_lines="$2"; shift 2 ;;
            -s|--side-by-side)      side_by_side=true; shift ;;
            -n|--no-color)          no_color=true; shift ;;
            --similarity)           show_similarity=true; shift ;;
            --summary)              summary_only=true; shift ;;
            -h|--help)              echo "$usage"; return 0 ;;
            -*)                     echo "Unknown option: $1" >&2; return 1 ;;
            *)                      [[ -z "$file1" ]] && file1="$1" || file2="$1"; shift ;;
        esac
    done
    
    [[ -z "$file1" || -z "$file2" ]] && { echo "$usage"; return 1; }
    [[ ! -f "$file1" ]] && { echo "âŒ File not found: $file1"; return 1; }
    [[ ! -f "$file2" ]] && { echo "âŒ File not found: $file2"; return 1; }
    
    echo -e "ğŸ” ${BASHRC_CYAN}Smart File Comparison${BASHRC_NC}"
    echo -e "ğŸ“„ ${BASHRC_YELLOW}File 1:${BASHRC_NC} $file1"
    echo -e "ğŸ“„ ${BASHRC_YELLOW}File 2:${BASHRC_NC} $file2"
    echo -e "ğŸ¯ ${BASHRC_YELLOW}Type:${BASHRC_NC} $comparison_type"
    
    # File information
    local file1_size=$(stat -c%s "$file1" 2>/dev/null || echo "0")
    local file2_size=$(stat -c%s "$file2" 2>/dev/null || echo "0")
    local file1_size_human=$(numfmt --to=iec-i --suffix=B "$file1_size" 2>/dev/null || echo "?")
    local file2_size_human=$(numfmt --to=iec-i --suffix=B "$file2_size" 2>/dev/null || echo "?")
    
    echo -e "ğŸ“ ${BASHRC_YELLOW}Sizes:${BASHRC_NC} $file1_size_human vs $file2_size_human"
    
    # Quick identical check
    if cmp -s "$file1" "$file2"; then
        echo -e "âœ… ${BASHRC_GREEN}Files are identical${BASHRC_NC}"
        return 0
    fi
    
    # Execute comparison based on type
    case "$comparison_type" in
        text)
            compare_text_intelligent "$file1" "$file2" "$ignore_case" "$ignore_whitespace" "$context_lines" "$side_by_side" "$no_color" "$show_similarity" "$summary_only"
            ;;
        binary)
            compare_binary_intelligent "$file1" "$file2" "$show_similarity" "$summary_only"
            ;;
        semantic)
            compare_semantic_intelligent "$file1" "$file2" "$show_similarity" "$summary_only"
            ;;
        structure)
            compare_structure_intelligent "$file1" "$file2" "$show_similarity" "$summary_only"
            ;;
        *)
            echo "âŒ Unknown comparison type: $comparison_type"
            return 1
            ;;
    esac
}

# Intelligent text comparison
compare_text_intelligent() {
    local file1="$1" file2="$2" ignore_case="$3" ignore_whitespace="$4" context="$5" side_by_side="$6" no_color="$7" similarity="$8" summary="$9"
    
    echo -e "\nğŸ“ ${BASHRC_CYAN}Text Comparison Results${BASHRC_NC}"
    echo -e "${BASHRC_CYAN}$(printf '=%.0s' {1..30})${BASHRC_NC}"
    
    # Build diff options
    local diff_opts=("-u$context")
    [[ "$ignore_case" == "true" ]] && diff_opts+=(-i)
    [[ "$ignore_whitespace" == "true" ]] && diff_opts+=(-w)
    [[ "$side_by_side" == "true" ]] && diff_opts=(-y)
    
    # Calculate similarity if requested
    if [[ "$similarity" == "true" ]]; then
        local total_lines1=$(wc -l < "$file1")
        local total_lines2=$(wc -l < "$file2")
        local different_lines=$(diff "${diff_opts[@]}" "$file1" "$file2" | grep -c "^[+-]" || echo "0")
        local similarity_percent=0
        
        if [[ $((total_lines1 + total_lines2)) -gt 0 ]]; then
            similarity_percent=$(echo "scale=1; 100 - ($different_lines * 100 / ($total_lines1 + $total_lines2))" | bc -l 2>/dev/null || echo "0")
        fi
        
        echo -e "ğŸ“Š ${BASHRC_YELLOW}Similarity:${BASHRC_NC} ${similarity_percent}%"
    fi
    
    if [[ "$summary" == "true" ]]; then
        # Show summary only
        local additions=$(diff -u "$file1" "$file2" | grep -c "^+" || echo "0")
        local deletions=$(diff -u "$file1" "$file2" | grep -c "^-" || echo "0")
        echo -e "â• ${BASHRC_GREEN}Additions:${BASHRC_NC} $additions lines"
        echo -e "â– ${BASHRC_RED}Deletions:${BASHRC_NC} $deletions lines"
    else
        # Show full diff with color if enabled
        if [[ "$no_color" == "false" && "$side_by_side" == "false" ]]; then
            diff "${diff_opts[@]}" "$file1" "$file2" | while IFS= read -r line; do
                case "${line:0:1}" in
                    "+") echo -e "${BASHRC_GREEN}$line${BASHRC_NC}" ;;
                    "-") echo -e "${BASHRC_RED}$line${BASHRC_NC}" ;;
                    "@") echo -e "${BASHRC_CYAN}$line${BASHRC_NC}" ;;
                    *) echo "$line" ;;
                esac
            done
        else
            diff "${diff_opts[@]}" "$file1" "$file2"
        fi
    fi
}

# Binary file comparison
compare_binary_intelligent() {
    local file1="$1" file2="$2" similarity="$3" summary="$4"
    
    echo -e "\nğŸ”¢ ${BASHRC_CYAN}Binary Comparison Results${BASHRC_NC}"
    echo -e "${BASHRC_CYAN}$(printf '=%.0s' {1..30})${BASHRC_NC}"
    
    # Use hexdump for comparison
    local temp1=$(mktemp) temp2=$(mktemp)
    
    if command -v xxd >/dev/null 2>&1; then
        xxd "$file1" > "$temp1"
        xxd "$file2" > "$temp2"
        
        local different_bytes=$(diff "$temp1" "$temp2" | wc -l)
        
        if [[ "$similarity" == "true" ]]; then
            local file1_size=$(stat -c%s "$file1")
            local similarity_percent=0
            if [[ $file1_size -gt 0 ]]; then
                similarity_percent=$(echo "scale=1; 100 - ($different_bytes * 100 / $file1_size)" | bc -l 2>/dev/null || echo "0")
            fi
            echo -e "ğŸ“Š ${BASHRC_YELLOW}Similarity:${BASHRC_NC} ${similarity_percent}%"
        fi
        
        if [[ "$summary" == "false" && $different_bytes -lt 100 ]]; then
            echo -e "ğŸ” ${BASHRC_YELLOW}Differences:${BASHRC_NC}"
            diff "$temp1" "$temp2" | head -20
        elif [[ "$summary" == "false" ]]; then
            echo -e "âš ï¸  ${BASHRC_YELLOW}Too many differences to display (${different_bytes} lines)${BASHRC_NC}"
        fi
        
        echo -e "ğŸ“Š ${BASHRC_YELLOW}Different bytes:${BASHRC_NC} $different_bytes"
    else
        echo -e "âŒ xxd not available for binary comparison"
    fi
    
    rm -f "$temp1" "$temp2"
}

# =============================================================================
# MODULE INITIALIZATION AND ALIASES
# =============================================================================

# Initialize the File Operations module
echo -e "${BASHRC_GREEN}âœ… File Operations Module Loaded${BASHRC_NC}"

# Create convenient aliases
alias org='organize'
alias dups='finddups'
alias watch='watchfiles'
alias sdiff='smartdiff'

# Advanced ls aliases with intelligence
alias ll='ls -lahF --color=auto --time-style=long-iso'
alias la='ls -A --color=auto'
alias l='ls -CF --color=auto'
alias lt='ls -lhrt --color=auto'  # Sort by time
alias lz='ls -lhrS --color=auto'  # Sort by size

# Smart directory operations
alias ..='cd ..'
alias ...='cd ../..'
alias ....='cd ../../..'
alias .....='cd ../../../..'

# Intelligent copy/move with progress
if command -v rsync >/dev/null 2>&1; then
    alias cp='rsync -ah --progress'
    alias cpx='rsync -ahx --progress'  # Don't cross filesystem boundaries
fi

# Safe operations
alias rm='rm -i'
alias mv='mv -i'
alias cp='cp -i'

# Create file operations directory for analytics
mkdir -p "$HOME/.bash_file_operations"/{monitoring,analytics,organize_history}

# Export all functions
export -f organize analyze_directory_for_organization organize_smart detect_file_category_by_content
export -f organize_by_type organize_by_date organize_by_size organize_by_project
export -f undo_organization learn_organization_patterns
export -f finddups parse_size find_duplicates_by_hash find_duplicates_by_size find_duplicates_by_name
export -f display_duplicates_text interactive_duplicate_deletion process_duplicate_group_interactive
export -f watchfiles update_monitoring_stats cleanup_file_monitoring
export -f smartdiff compare_text_intelligent compare_binary_intelligent compare_semantic_intelligent

echo -e "${BASHRC_PURPLE}ğŸ‰ Ultimate File Operations v$FILE_OPS_MODULE_VERSION Ready!${BASHRC_NC}"
echo -e "${BASHRC_CYAN}ğŸ’¡ Try: 'organize --help', 'finddups --help', 'watchfiles --help', 'smartdiff --help'${BASHRC_NC}"
